{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ayushi\n",
    "# import csv\n",
    "\n",
    "# csv_path = '/Users/ayushic98/Desktop/Assignment 3/lotte/lifestyle/dev/collection.tsv'\n",
    "# with open(csv_path) as file:\n",
    "\n",
    "#     collection_passages = csv.reader(file, delimiter = '\\t')\n",
    "\n",
    "# for line in collection_passages:\n",
    "#     print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added by ayushi\n",
    "# import json\n",
    "\n",
    "#opening json file\n",
    "# json_path = '/Users/ayushic98/Desktop/Assignment 3/lotte/lifestyle/dev/qas.forum.jsonl'\n",
    "# forum_qas_json = open(json_path)\n",
    "\n",
    "#returns json object as a dictionary\n",
    "#forum_qas = json.load(forum_qas_json.read())\n",
    "\n",
    "# for i in forum_qas_json:\n",
    "#     print(i)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lifestyle *dev* and *test* dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata of lifestyle dev dataset: \n",
      "{'docs': {'count': 268893, 'fields': {'doc_id': {'max_len': 6, 'common_prefix': ''}}}} \n",
      "\n",
      "---------------------------------------------------------------------------------- \n",
      "\n",
      "Metadata of lifestyle test dataset: \n",
      "{'docs': {'count': 119461, 'fields': {'doc_id': {'max_len': 6, 'common_prefix': ''}}}} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "\n",
    "lifestyle_dev = ir_datasets.load(name = \"lotte/lifestyle/dev\")\n",
    "lifestyle_test = ir_datasets.load(name = \"lotte/lifestyle/test\")\n",
    "\n",
    "print(\"Metadata of lifestyle dev dataset: \")\n",
    "print(lifestyle_dev.metadata(), \"\\n\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------- \\n\")\n",
    "\n",
    "print(\"Metadata of lifestyle test dataset: \")\n",
    "print(lifestyle_test.metadata(), \"\\n\")\n",
    "\n",
    "# for doc in lifestyle_dev.docs_iter():\n",
    "    # print(doc)\n",
    "\n",
    "# print(\"Number of documents in the lifestyle dev dataset: \", lifestyle_dev.docs_count(), \"\\n\")\n",
    "\n",
    "# print(\"Number of documents in the lifestyle test dataset: \", lifestyle_test.docs_count())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lifestyle dev *forum* dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lifestyle dev dataset: forum docs, queries, and qrels:- \n",
      "\n",
      "Metadata about the docs in this dataset: \n",
      "{'count': 268893, 'fields': {'doc_id': {'max_len': 6, 'common_prefix': ''}}} \n",
      "\n",
      "Count of documents = 268893 \n",
      "\n",
      "Count of forum queries = 2076 \n",
      "\n",
      "{1: 'Answer upvoted or accepted on stack exchange'}\n",
      "Count of qrels = 12823\n"
     ]
    }
   ],
   "source": [
    "print(\"lifestyle dev dataset: forum docs, queries, and qrels:- \\n\")\n",
    "forum_dev = ir_datasets.load(name = \"lotte/lifestyle/dev/forum\")\n",
    "\n",
    "print(\"Metadata about the docs in this dataset: \")\n",
    "print(forum_dev.docs_metadata(), \"\\n\")\n",
    "\n",
    "# working on DOCUMENTS from lotte/lifestyle/dev/forum\n",
    "# for forum_doc in forum_dev.docs_iter():\n",
    "    # print(forum_doc)\n",
    "\n",
    "print(\"Count of documents =\", forum_dev.docs_count(), \"\\n\")\n",
    "\n",
    "# working on QUERIES from lotte/lifestyle/dev/forum\n",
    "# for forum_query in forum_dev.queries_iter():\n",
    "    # print(forum_query)\n",
    "\n",
    "print(\"Count of forum queries =\", forum_dev.queries_count(), \"\\n\")\n",
    "\n",
    "#working on RELEVANCE LEVELS from lotte/lifestyle/dev/forum\n",
    "# for forum_qrel in forum_dev.qrels_iter():\n",
    "    # print(forum_qrel)\n",
    "\n",
    "# print(\"\\n\")\n",
    "print(forum_dev.qrels_defs())\n",
    "print(\"Count of qrels =\", forum_dev.qrels_count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lifestyle dev *search* dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lifestyle dev dataset: search docs, queries, and qrels:- \n",
      "\n",
      "Metadata about the docs in this dataset: \n",
      "{'count': 268893, 'fields': {'doc_id': {'max_len': 6, 'common_prefix': ''}}} \n",
      "\n",
      "Count of documents = 268893 \n",
      "\n",
      "Count of search queries = 417 \n",
      "\n",
      "{1: 'Answer upvoted or accepted on stack exchange'}\n",
      "Count of qrels = 1376\n"
     ]
    }
   ],
   "source": [
    "print(\"lifestyle dev dataset: search docs, queries, and qrels:- \\n\")\n",
    "search_dev = ir_datasets.load(name = \"lotte/lifestyle/dev/search\")\n",
    "\n",
    "print(\"Metadata about the docs in this dataset: \")\n",
    "print(search_dev.docs_metadata(), \"\\n\")\n",
    "\n",
    "# working on DOCUMENTS from lotte/lifestyle/dev/search\n",
    "# for search_doc in search_dev.docs_iter():\n",
    "    # print(search_doc)\n",
    "\n",
    "print(\"Count of documents =\", search_dev.docs_count(), \"\\n\")\n",
    "\n",
    "# working on QUERIES from lotte/lifestyle/dev/search\n",
    "# for search_query in search_dev.queries_iter():\n",
    "    # print(search_query)\n",
    "\n",
    "print(\"Count of search queries =\", search_dev.queries_count(), \"\\n\")\n",
    "\n",
    "#working on RELEVANCE LEVELS from lotte/lifestyle/dev/search\n",
    "# for search_qrel in search_dev.qrels_iter():\n",
    "#     print(search_qrel)\n",
    "\n",
    "# print(\"\\n\")\n",
    "print(search_dev.qrels_defs())\n",
    "print(\"Count of qrels =\", search_dev.qrels_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Foods to avoid feeding your rabbit: Beets (sugary), Breakfast cereals, Chocolate (NEVER give this to any pet - it is poisonous to most), Corn (rabbits can't digest the hulls of the corn kernels), Diatomaceous Earth - this is made from finely ground shells, and even the highest quality can have edges which act like razor blades against the thin lining of a rabbits stomach lining. Do not use for food or litter,Fresh peas, Grains, Green beans (can cause gas), Iceberg lettuce (and any light green lettuce leaves - they are high in water content but low in nutrients and may cause diahrrea) - can be gven in small amounts or when you have no other greens to offer, but watch the output for soft stools, Legumes, Nuts, Onions, Pelleted food with seeds mixed in (it's a treat to the bunnies but the seeds cause stomach problems), Potatoes (white or red), Seeds of any kind, Starches of any kind, Sugar, in any form (small quantities of treats are allowed, but no more than a tablespoon of fruit or raisins or anything containing any types of sugar) Foods that are toxic to rabbits: Note: do not allow your rabbit to eat dried leaves from any trees - too many types are very toxic and some can cause cyanide poisoning (this includes specifically apple tree leaves, oak leaves, maple leaves, etc). If you allow your rabbit an outdoor run, please clear the area of leaves as the rabbit may find them very tasty, but they are very dangerous!! Foods that contain compounds that destroy nutrients: Sweet potato, Cassava, Bamboo shoots, Maize, Lima beans, Millet, Bracken fern, Tea leaves, Coffee plants. Generally toxic: Rhubarb leaves, Raw lima, kidney or soy beans, Onions, Citrus peels. Oxalates (causes pain and swelling of mouth and throat, swollen tissue can restrict breathing or cause suffocation.) Begonia, Caladium, Calla lily, Diffenbachia, Dumbcane, Jack in the pulpit, Philodendron, Schefflera. Minor Toxicities (causes vomiting, diarrhea, nausea) Aloe vera, Amaryllis, Bird of paradise, Birch, Boxwood Cedar, Chrysanthemum, Daffodil, Daisy, Eucalyptus, Galiola, Hydrangea, Haycinth, Iris, Juniper, Redwood tree, Rananculus, Sweet pea, Sweet william, Violas. Extremely Toxic (one leaf can kill) Angels Trumpet, Azalea, Black Acacia and Locast, Bleeding Heart, Carmellia, Carnation, Carolinia Jasmine, Castor Beans, Christmas Beans, China Berry, Clementis, Coffee Tree Plant, Cyclamen, Daphne, Delphinium, Easter Lily, Elderberry, Flax, Four-o-clocks, Geranium, Heavenly Bamboo, Hemlock, Holly Berries, Ivy, Jerusalem cherry, Lantana, Larkspur, Licorice plant, Lily of the valley, Lobelia, Milkvetch, Monkshood, Morning glory, Mountain laurel, Narcissus, Nightshade Loeander, Pea family, Pig weed, Potato plant, Pivet, Rhododendron, String of pearls, Thorn apple, Toyon, Vinca, Wintergreen, Wisteria, Tew. From: http://board.smallanimalchannel.com/Topic9319.aspx Bamboo shoots when raw have cyanide. Cyanide is highly toxic and results in rapid breathing, pale extremities, coma, and possibly death. The toxins can depend from plant to plant but I would not risk it. The ones sold for human consumption need to be cooked/treated to release the toxins. It is not listed as a safe food for bunnies. From: http://www.rabbitsonline.net/showthread.php?t=66688 Hope this helps. Did extensive research and I found the best articles/websites that would hopefully help.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#docs_store() helps fast lookups by doc_id\n",
    "docstore = lifestyle_dev.docs_store()\n",
    "docstore.get('2501').text\n",
    "# \"John Maynard Keynes, 1st Baron Keynes, CB, FBA (/ˈkeɪnz/ KAYNZ; 5 June 1883 – 21 April [SNIP]\"\n",
    "# Naïve UTF-8 decoding yields double-encoding artifacts like:\n",
    "# \"John Maynard Keynes, 1st Baron Keynes, CB, FBA (/Ë\\x88keÉªnz/ KAYNZ; 5 June 1883 â\\x80\\x93 21 April [SNIP]\"\n",
    "#                                                  ~~~~~~  ~~                       ~~~~~~~~~"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lifestyle test *forum* dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lifestyle test dataset: forum docs, queries, and qrels:- \n",
      "\n",
      "Metadata about the docs in this dataset: \n",
      "{'count': 119461, 'fields': {'doc_id': {'max_len': 6, 'common_prefix': ''}}} \n",
      "\n",
      "Count of documents = 119461 \n",
      "\n",
      "Count of forum queries = 2002 \n",
      "\n",
      "{1: 'Answer upvoted or accepted on stack exchange'}\n",
      "Count of qrels = 10278\n"
     ]
    }
   ],
   "source": [
    "print(\"lifestyle test dataset: forum docs, queries, and qrels:- \\n\")\n",
    "forum_test = ir_datasets.load(name = \"lotte/lifestyle/test/forum\")\n",
    "\n",
    "print(\"Metadata about the docs in this dataset: \")\n",
    "print(forum_test.docs_metadata(), \"\\n\")\n",
    "\n",
    "# working on DOCUMENTS from lotte/lifestyle/test/forum\n",
    "# for forum_doc in forum_test.docs_iter():\n",
    "    # print(forum_doc)\n",
    "\n",
    "print(\"Count of documents =\", forum_test.docs_count(), \"\\n\")\n",
    "\n",
    "# working on QUERIES from lotte/lifestyle/test/forum\n",
    "# for forum_query in forum_test.queries_iter():\n",
    "    # print(forum_query)\n",
    "\n",
    "print(\"Count of forum queries =\", forum_test.queries_count(), \"\\n\")\n",
    "\n",
    "#working on RELEVANCE LEVELS from lotte/lifestyle/test/forum\n",
    "# for forum_qrel in forum_test.qrels_iter():\n",
    "    # print(forum_qrel)\n",
    "\n",
    "# print(\"\\n\")\n",
    "print(forum_test.qrels_defs())\n",
    "print(\"Count of qrels =\", forum_test.qrels_count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lifestyle test *search* dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lifestyle test dataset: search docs, queries, and qrels:- \n",
      "\n",
      "Metadata about the docs in this dataset: \n",
      "{'count': 119461, 'fields': {'doc_id': {'max_len': 6, 'common_prefix': ''}}} \n",
      "\n",
      "Count of documents = 119461 \n",
      "\n",
      "Count of search queries = 661 \n",
      "\n",
      "{1: 'Answer upvoted or accepted on stack exchange'}\n",
      "Count of qrels = 1804\n"
     ]
    }
   ],
   "source": [
    "print(\"lifestyle test dataset: search docs, queries, and qrels:- \\n\")\n",
    "search_test = ir_datasets.load(name = \"lotte/lifestyle/test/search\")\n",
    "\n",
    "print(\"Metadata about the docs in this dataset: \")\n",
    "print(search_test.docs_metadata(), \"\\n\")\n",
    "\n",
    "# working on DOCUMENTS from lotte/lifestyle/test/search\n",
    "# for search_doc in search_test.docs_iter():\n",
    "    # print(search_doc)\n",
    "\n",
    "print(\"Count of documents =\", search_test.docs_count(), \"\\n\")\n",
    "\n",
    "# working on QUERIES from lotte/lifestyle/test/search\n",
    "# for search_query in search_test.queries_iter():\n",
    "    # print(search_query)\n",
    "\n",
    "print(\"Count of search queries =\", search_test.queries_count(), \"\\n\")\n",
    "\n",
    "#working on RELEVANCE LEVELS from lotte/lifestyle/test/search\n",
    "# for search_qrel in search_test.qrels_iter():\n",
    "#     print(search_qrel)\n",
    "\n",
    "# print(\"\\n\")\n",
    "print(search_test.qrels_defs())\n",
    "print(\"Count of qrels =\", search_test.qrels_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -q https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/lotte.tar.gz\n",
    "# !tar -xzf lotte.tar.gz\n",
    "# !sudo chown -R daemon:daemon lotte/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ir_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lotte -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze | grep ir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utility packages\n",
    "# import urllib.request \n",
    "# from bs4 import BeautifulSoup \n",
    "# import re\n",
    "# import time\n",
    "\n",
    "# let's import ES\n",
    "# from loTTE import Lifestyle\n",
    "# from lotte import lifestyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import codecs\n",
    "# from typing import NamedTuple, Dict, List\n",
    "# import ir_datasets\n",
    "# from ir_datasets.util import TarExtractAll, Cache, RelativePath, Lazy, Migrator\n",
    "# from ir_datasets.datasets.base import Dataset, YamlDocumentation, FilteredQueries\n",
    "# from ir_datasets.formats import TsvDocs, TsvQueries, BaseQrels, GenericDoc, GenericQuery, TrecQrel\n",
    "# from ir_datasets.indices import PickleLz4FullStore\n",
    "\n",
    "# _logger = ir_datasets.log.easy()\n",
    "\n",
    "\n",
    "# NAME = 'lotte'\n",
    "# QRELS_DEFS = {1: 'Answer upvoted or accepted on stack exchange'}\n",
    "\n",
    "\n",
    "\n",
    "# class LotteQrels(BaseQrels):\n",
    "#     def __init__(self, qrels_dlc):\n",
    "#         self._qrels_dlc = qrels_dlc\n",
    "\n",
    "#     def qrels_path(self):\n",
    "#         return self._qrels_dlc.path()\n",
    "\n",
    "#     def qrels_iter(self):\n",
    "#         with self._qrels_dlc.stream() as f:\n",
    "#             for line in f:\n",
    "#                 data = json.loads(line)\n",
    "#                 for did in data['answer_pids']:\n",
    "#                     yield TrecQrel(str(data['qid']), str(did), 1, \"0\")\n",
    "\n",
    "#     def qrels_cls(self):\n",
    "#         return TrecQrel\n",
    "\n",
    "#     def qrels_defs(self):\n",
    "#         return QRELS_DEFS\n",
    "\n",
    "\n",
    "# def _init():\n",
    "#     base_path = ir_datasets.util.home_path()/NAME\n",
    "#     dlc = ir_datasets.util.DownloadConfig.context(NAME, base_path)\n",
    "#     documentation = YamlDocumentation(f'docs/{NAME}.yaml')\n",
    "\n",
    "#     base_dlc = TarExtractAll(dlc['source'], base_path/'lotte_extracted')\n",
    "\n",
    "#     base = Dataset(documentation('_'))\n",
    "\n",
    "#     subsets = {}\n",
    "\n",
    "#     domains = [\n",
    "#         ('lifestyle',),\n",
    "#         ('recreation',),\n",
    "#         ('science',),\n",
    "#         ('technology',),\n",
    "#         ('writing',),\n",
    "#         ('pooled',),\n",
    "#     ]\n",
    "\n",
    "#     for (domain,) in domains:\n",
    "#         for split in ['dev', 'test']:\n",
    "#             corpus = TsvDocs(RelativePath(base_dlc, f'lotte/{domain}/{split}/collection.tsv'), lang='en')\n",
    "#             subsets[f'{domain}/{split}'] = Dataset(\n",
    "#                 corpus,\n",
    "#                 documentation(f'{domain}/{split}')\n",
    "#             )\n",
    "#             for qtype in ['search', 'forum']:\n",
    "#                 subsets[f'{domain}/{split}/{qtype}'] = Dataset(\n",
    "#                     corpus,\n",
    "#                     TsvQueries(RelativePath(base_dlc, f'lotte/{domain}/{split}/questions.{qtype}.tsv'), lang='en'),\n",
    "#                     LotteQrels(RelativePath(base_dlc, f'lotte/{domain}/{split}/qas.{qtype}.jsonl')),\n",
    "#                     documentation(f'{domain}/{split}/{qtype}')\n",
    "#                 )\n",
    "\n",
    "#     # ir_datasets.registry.register(NAME, base)\n",
    "#     # for s in sorted(subsets):\n",
    "#     #     ir_datasets.registry.register(f'{NAME}/{s}', subsets[s])\n",
    "\n",
    "#     return base, subsets\n",
    "\n",
    "\n",
    "# base, subsets = _init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Copyright 2020 The HuggingFace Datasets Authors and the current dataset script contributor.\n",
    "# #\n",
    "# # Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# # you may not use this file except in compliance with the License.\n",
    "# # You may obtain a copy of the License at\n",
    "# #\n",
    "# #     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# #\n",
    "# # Unless required by applicable law or agreed to in writing, software\n",
    "# # distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# # See the License for the specific language governing permissions and\n",
    "# # limitations under the License.\n",
    "# # Address all TODOs and remove all explanatory comments\n",
    "# \"\"\"Add a description here.\"\"\"\n",
    "\n",
    "\n",
    "# import csv\n",
    "# import json\n",
    "# import os\n",
    "\n",
    "# import datasets\n",
    "# import pandas as pd\n",
    "# import zipfile\n",
    "\n",
    "\n",
    "# # Add BibTeX citation\n",
    "# # Find for instance the citation on arxiv or on the dataset repo/website\n",
    "# _CITATION = \"\"\"\\\n",
    "# @inproceedings{santhanam-etal-2022-colbertv2,\n",
    "#     title = \"{C}ol{BERT}v2: Effective and Efficient Retrieval via Lightweight Late Interaction\",\n",
    "#     author = \"Santhanam, Keshav  and\n",
    "#       Khattab, Omar  and\n",
    "#       Saad-Falcon, Jon  and\n",
    "#       Potts, Christopher  and\n",
    "#       Zaharia, Matei\",\n",
    "#     booktitle = \"Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n",
    "#     month = jul,\n",
    "#     year = \"2022\",\n",
    "#     address = \"Seattle, United States\",\n",
    "#     publisher = \"Association for Computational Linguistics\",\n",
    "#     url = \"https://aclanthology.org/2022.naacl-main.272\",\n",
    "#     pages = \"3715--3734\",\n",
    "#     abstract = \"Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce Maize, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate Maize across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6{--}10x.\",\n",
    "# }\n",
    "# \"\"\"\n",
    "\n",
    "# # Add description of the dataset here\n",
    "# # You can copy an official description\n",
    "# _DESCRIPTION = \"\"\"\\\n",
    "# LoTTE Passages Dataset for ColBERTv2\n",
    "# \"\"\"\n",
    "\n",
    "# # Add a link to an official homepage for the dataset here\n",
    "# _HOMEPAGE = \"https://huggingface.co/datasets/colbertv2/lotte\"\n",
    "\n",
    "# # Add the licence for the dataset here if you can find it\n",
    "# _LICENSE = \"\"\n",
    "\n",
    "# _URL = {\n",
    "#             \"pooled\": \"https://huggingface.co/datasets/colbertv2/lotte/resolve/main/pooled/\",\n",
    "#             \"lifestyle\": \"https://huggingface.co/datasets/colbertv2/lotte/resolve/main/lifestyle/\",\n",
    "#             \"recreation\": \"https://huggingface.co/datasets/colbertv2/lotte/resolve/main/recreation/\",\n",
    "#             \"science\": \"https://huggingface.co/datasets/colbertv2/lotte/resolve/main/science/\",\n",
    "#             \"technology\": \"https://huggingface.co/datasets/colbertv2/lotte/resolve/main/technology/\",\n",
    "#             \"writing\": \"https://huggingface.co/datasets/colbertv2/lotte/resolve/main/writing/\"\n",
    "#        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Name of the dataset usually match the script name with CamelCase instead of snake_case\n",
    "# class NewDataset(datasets.GeneratorBasedBuilder):\n",
    "#     \"\"\"Short description of my dataset.\"\"\"\n",
    "\n",
    "#     VERSION = datasets.Version(\"1.1.0\")\n",
    "\n",
    "#     # This is an example of a dataset with multiple configurations.\n",
    "#     # If you don't want/need to define several sub-sets in your dataset,\n",
    "#     # just remove the BUILDER_CONFIG_CLASS and the BUILDER_CONFIGS attributes.\n",
    "\n",
    "#     # If you need to make complex sub-parts in the datasets with configurable options\n",
    "#     # You can create your own builder configuration class to store attribute, inheriting from datasets.BuilderConfig\n",
    "#     # BUILDER_CONFIG_CLASS = MyBuilderConfig\n",
    "\n",
    "#     # You will be able to load one or the other configurations in the following list with\n",
    "#     # data = datasets.load_dataset('my_dataset', 'first_domain')\n",
    "#     # data = datasets.load_dataset('my_dataset', 'second_domain')\n",
    "#     BUILDER_CONFIGS = [\n",
    "#         datasets.BuilderConfig(name=\"pooled\", version=VERSION, description=\"\"),\n",
    "#         datasets.BuilderConfig(name=\"lifestyle\", version=VERSION, description=\"\"),\n",
    "#         datasets.BuilderConfig(name=\"recreation\", version=VERSION, description=\"\"),\n",
    "#         datasets.BuilderConfig(name=\"science\", version=VERSION, description=\"\"),\n",
    "#         datasets.BuilderConfig(name=\"technology\", version=VERSION, description=\"\"),\n",
    "#         datasets.BuilderConfig(name=\"writing\", version=VERSION, description=\"\"),\n",
    "#         #datasets.BuilderConfig(name=\"pooled_search_valid\", version=VERSION, description=\"\"),\n",
    "#         #datasets.BuilderConfig(name=\"pooled_search_test\", version=VERSION, description=\"\"),\n",
    "#         #datasets.BuilderConfig(name=\"pooled_forum_valid\", version=VERSION, description=\"\"),\n",
    "#         #datasets.BuilderConfig(name=\"pooled_forum_test\", version=VERSION, description=\"\"),\n",
    "#         #datasets.BuilderConfig(name=\"lifestyle_search\", version=VERSION, description=\"\"),\n",
    "#         #datasets.BuilderConfig(name=\"lifestyle_forum\", version=VERSION, description=\"\"),\n",
    "#         #datasets.BuilderConfig(name=\"recreation_search\", version=VERSION, description=\"\"),\n",
    "#         #datasets.BuilderConfig(name=\"recreation_forum\", version=VERSION, description=\"\"),\n",
    "#         #datasets.BuilderConfig(name=\"science_search\", version=VERSION, description=\"\"),\n",
    "#         #datasets.BuilderConfig(name=\"science_forum\", version=VERSION, description=\"\"),\n",
    "#         #datasets.BuilderConfig(name=\"technology_search\", version=VERSION, description=\"\"),\n",
    "#         #datasets.BuilderConfig(name=\"technology_forum\", version=VERSION, description=\"\"),\n",
    "#         #datasets.BuilderConfig(name=\"writing_search\", version=VERSION, description=\"\"),\n",
    "#         #datasets.BuilderConfig(name=\"writing_forum\", version=VERSION, description=\"\"),\n",
    "#     ]\n",
    "\n",
    "#     DEFAULT_CONFIG_NAME = \"pooled\"  # It's not mandatory to have a default configuration. Just use one if it make sense.\n",
    "\n",
    "#     def _info(self):\n",
    "#         # This method specifies the datasets.DatasetInfo object which contains informations and typings for the dataset\n",
    "#         features = datasets.Features(\n",
    "#             {\n",
    "#                 \"qid\": datasets.Value(\"int32\"),\n",
    "#                 \"query\": datasets.Value(\"string\"),\n",
    "#                 \"author\": datasets.Value(\"string\"),\n",
    "#                 \"answers\": datasets.Sequence(feature={\"views\": datasets.Value(dtype='int32', id=None),\n",
    "#                                                       \"score\": datasets.Value(dtype='int32', id=None),\n",
    "#                                                       \"answer_pids\": datasets.Value(dtype='int32', id=None)})\n",
    "#             }\n",
    "#         )\n",
    "#         return datasets.DatasetInfo(\n",
    "#             # This is the description that will appear on the datasets page.\n",
    "#             description=_DESCRIPTION,\n",
    "#             # This defines the different columns of the dataset and their types\n",
    "#             features=features,  # Here we define them above because they are different between the two configurations\n",
    "#             # If there's a common (input, target) tuple from the features, uncomment supervised_keys line below and\n",
    "#             # specify them. They'll be used if as_supervised=True in builder.as_dataset.\n",
    "#             # supervised_keys=(\"sentence\", \"label\"),\n",
    "#             # Homepage of the dataset for documentation\n",
    "#             homepage=_HOMEPAGE,\n",
    "#             # License for the dataset if available\n",
    "#             license=_LICENSE,\n",
    "#             # Citation for the dataset\n",
    "#             citation=_CITATION,\n",
    "#         )\n",
    "\n",
    "#     def _split_generators(self, dl_manager):\n",
    "#         # This method is tasked with downloading/extracting the data and defining the splits depending on the configuration\n",
    "#         # If several configurations are possible (listed in BUILDER_CONFIGS), the configuration selected by the user is in self.config.name\n",
    "\n",
    "#         # dl_manager is a datasets.download.DownloadManager that can be used to download and extract URLS\n",
    "#         # It can accept any type or nested list/dict and will give back the same structure with the url replaced with path to local files.\n",
    "#         # By default the archives will be extracted and a path to a cached folder where they are extracted is returned instead of the archive\n",
    "        \n",
    "#         _URLS = {\n",
    "#             \"forum_dev\": _URL[self.config.name] + \"dev_qas.forum.jsonl\",\n",
    "#             \"forum_test\": _URL[self.config.name] + \"test_qas.forum.jsonl\",\n",
    "#             \"search_dev\": _URL[self.config.name] + \"dev_qas.search.jsonl\",\n",
    "#             \"search_test\": _URL[self.config.name] + \"test_qas.search.jsonl\",\n",
    "#         }\n",
    "\n",
    "#         downloaded_files = dl_manager.download_and_extract(_URLS)\n",
    "\n",
    "#         return [\n",
    "#             datasets.SplitGenerator(name=\"forum_dev\", gen_kwargs={\"filepath\": downloaded_files[\"forum_dev\"]}),\n",
    "#             datasets.SplitGenerator(name=\"forum_test\", gen_kwargs={\"filepath\": downloaded_files[\"forum_test\"]}),\n",
    "#             datasets.SplitGenerator(name=\"search_dev\", gen_kwargs={\"filepath\": downloaded_files[\"search_dev\"]}),\n",
    "#             datasets.SplitGenerator(name=\"search_test\", gen_kwargs={\"filepath\": downloaded_files[\"search_test\"]}),\n",
    "#         ]\n",
    "\n",
    "#     # method parameters are unpacked from `gen_kwargs` as given in `_split_generators`\n",
    "#     def _generate_examples(self, filepath):\n",
    "#         # This method handles input defined in _split_generators to yield (key, example) tuples from the dataset.\n",
    "#         # The `key` is for legacy reasons (tfds) and is not important in itself, but must be unique for each example.\n",
    "\n",
    "#         print(\"Generating examples\")\n",
    "\n",
    "#         # with open(filepath, encoding=\"utf-8\") as f:\n",
    "#         with open(filepath, errors=\"ignore\", encoding=\"utf-8\") as f:\n",
    "\n",
    "#             for key, row in enumerate(f):\n",
    "                \n",
    "#                 data = json.loads(row)\n",
    "\n",
    "#                 answers = {}\n",
    "\n",
    "#                 if \"score\" in data.keys():\n",
    "#                     answers.update({\"score\": [data['score']]})\n",
    "#                 else:\n",
    "#                     answers.update({\"score\": []})\n",
    "\n",
    "#                 if \"views\" in data.keys():\n",
    "#                     answers.update({\"views\": [data['views']]})\n",
    "#                 else:\n",
    "#                     answers.update({\"views\": []})\n",
    "\n",
    "#                 answers.update({\"answer_pids\": data['answer_pids']})\n",
    "\n",
    "#                 if \"question_author\" in data.keys():\n",
    "#                     author = data['question_author']\n",
    "#                 else:\n",
    "#                     author = \"\"\n",
    "\n",
    "\n",
    "#                 yield key, {\n",
    "#                         \"qid\": data[\"qid\"],\n",
    "#                         \"query\": data[\"query\"],\n",
    "#                         \"author\": author,\n",
    "#                         \"answers\": answers\n",
    "#                 }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ir_datasets\n",
    "# dataset = ir_datasets.load('lotte/lifestyle/dev')\n",
    "# # dataset = ir_datasets\n",
    "# # # Documents\n",
    "# for doc in dataset.docs_iter():\n",
    "#     print(doc)\n",
    "# # GenericDoc(doc_id='0', text='The presence of communication amid scientific minds was equa...\n",
    "# # GenericDoc(doc_id='1', text='The Manhattan Project and its atomic bomb helped bring an en...\n",
    "# # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ir_datasets\n",
    "# dataset = ir_datasets.load(\"lotte/lifestyle/dev/search\")\n",
    "# for query in dataset.queries_iter():\n",
    "#      query # namedtuple<query_id, text>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query\n",
    "# query[0]\n",
    "# query_id[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
