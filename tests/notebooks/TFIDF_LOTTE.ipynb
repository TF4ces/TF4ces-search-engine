{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Step_1: Loading the dataset"
      ],
      "metadata": {
        "id": "APlxiOzwwyCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/lotte.tar.gz\n",
        "!tar -xzf lotte.tar.gz\n",
        "!sudo chown -R daemon:daemon lotte/"
      ],
      "metadata": {
        "id": "gFIYmzSgf9XF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lotte -q"
      ],
      "metadata": {
        "id": "zEk7G1G0chTf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1c1b20d-c255-4aa2-c7b7-094e66f0b3c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/2.2 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for yattag (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze | grep lotte"
      ],
      "metadata": {
        "id": "RyKI2_isfmPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request \n",
        "from bs4 import BeautifulSoup \n",
        "import re\n",
        "import time\n",
        "\n",
        "# let's import ES\n",
        "# from elasticsearch import Elasticsearch\n",
        "from lotte import lifestyle"
      ],
      "metadata": {
        "id": "CcH3pTQUf4db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash --bg\n",
        "sudo -H -u daemon lotte/bin/lotte\n",
        "\n",
        "%%bash\n",
        "ps -ef | grep lotte"
      ],
      "metadata": {
        "id": "bwmVkyCzf7F5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ir_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KR53vBcq52h",
        "outputId": "f5776b7c-fbe7-4f3e-b654-2f600c623619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ir_datasets\n",
            "  Downloading ir_datasets-0.5.4-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.5/311.5 KB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting warc3-wet>=0.2.3\n",
            "  Downloading warc3_wet-0.2.3-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.9/dist-packages (from ir_datasets) (2.27.1)\n",
            "Collecting trec-car-tools>=2.5.4\n",
            "  Downloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
            "Collecting zlib-state>=0.1.3\n",
            "  Downloading zlib_state-0.1.5-cp39-cp39-manylinux2010_x86_64.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unlzw3>=0.2.1\n",
            "  Downloading unlzw3-0.2.2-py3-none-any.whl (6.1 kB)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.9/dist-packages (from ir_datasets) (1.22.4)\n",
            "Collecting lz4>=3.1.1\n",
            "  Downloading lz4-4.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ijson>=3.1.3\n",
            "  Downloading ijson-3.2.0.post0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 KB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyautocorpus>=0.1.1\n",
            "  Downloading pyautocorpus-0.1.9-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (293 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 KB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.9/dist-packages (from ir_datasets) (4.11.2)\n",
            "Requirement already satisfied: tqdm>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from ir_datasets) (4.65.0)\n",
            "Collecting warc3-wet-clueweb09>=0.2.5\n",
            "  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.9/dist-packages (from ir_datasets) (4.9.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.9/dist-packages (from ir_datasets) (6.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4>=4.4.1->ir_datasets) (2.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->ir_datasets) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->ir_datasets) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->ir_datasets) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->ir_datasets) (2022.12.7)\n",
            "Collecting cbor>=1.0.0\n",
            "  Downloading cbor-1.0.0.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: warc3-wet-clueweb09, cbor\n",
            "  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18918 sha256=973dadc5e40f02ae459da6b41a96cfacdec965e6ecdc907e686c54a59decb510\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/22/ed/a11944d7fdf4e94c4206a3f760d385122a4d34d8acc12f71a3\n",
            "  Building wheel for cbor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cbor: filename=cbor-1.0.0-cp39-cp39-linux_x86_64.whl size=57301 sha256=428d8568f8c4f60b92e25a4e9af75b06b9988207c353a6fb0fa9193571d2773c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/10/03/a281e0682ddd4b310431fb25d1a4f53987105267cf46c417f3\n",
            "Successfully built warc3-wet-clueweb09 cbor\n",
            "Installing collected packages: warc3-wet-clueweb09, warc3-wet, ijson, cbor, zlib-state, unlzw3, trec-car-tools, pyautocorpus, lz4, ir_datasets\n",
            "Successfully installed cbor-1.0.0 ijson-3.2.0.post0 ir_datasets-0.5.4 lz4-4.3.2 pyautocorpus-0.1.9 trec-car-tools-2.6 unlzw3-0.2.2 warc3-wet-0.2.3 warc3-wet-clueweb09-0.2.5 zlib-state-0.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqybSxiIq-ko",
        "outputId": "2d63ed5f-c5aa-4c30-b7a2-1697020fde23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Collecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.2.0\n",
            "  Downloading huggingface_hub-0.13.2-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.4)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.10.1 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.13.2 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import codecs\n",
        "from typing import NamedTuple, Dict, List\n",
        "import ir_datasets\n",
        "from ir_datasets.util import TarExtractAll, Cache, RelativePath, Lazy, Migrator\n",
        "from ir_datasets.datasets.base import Dataset, YamlDocumentation, FilteredQueries\n",
        "from ir_datasets.formats import TsvDocs, TsvQueries, BaseQrels, GenericDoc, GenericQuery, TrecQrel\n",
        "from ir_datasets.indices import PickleLz4FullStore\n",
        "\n",
        "_logger = ir_datasets.log.easy()\n",
        "\n",
        "\n",
        "NAME = 'lotte'\n",
        "QRELS_DEFS = {1: 'Answer upvoted or accepted on stack exchange'}\n",
        "\n",
        "\n",
        "\n",
        "class LotteQrels(BaseQrels):\n",
        "    def __init__(self, qrels_dlc):\n",
        "        self._qrels_dlc = qrels_dlc\n",
        "\n",
        "    def qrels_path(self):\n",
        "        return self._qrels_dlc.path()\n",
        "\n",
        "    def qrels_iter(self):\n",
        "        with self._qrels_dlc.stream() as f:\n",
        "            for line in f:\n",
        "                data = json.loads(line)\n",
        "                for did in data['answer_pids']:\n",
        "                    yield TrecQrel(str(data['qid']), str(did), 1, \"0\")\n",
        "\n",
        "    def qrels_cls(self):\n",
        "        return TrecQrel\n",
        "\n",
        "    def qrels_defs(self):\n",
        "        return QRELS_DEFS\n",
        "\n",
        "\n",
        "def _init():\n",
        "    base_path = ir_datasets.util.home_path()/NAME\n",
        "    dlc = ir_datasets.util.DownloadConfig.context(NAME, base_path)\n",
        "    documentation = YamlDocumentation(f'docs/{NAME}.yaml')\n",
        "\n",
        "    base_dlc = TarExtractAll(dlc['source'], base_path/'lotte_extracted')\n",
        "\n",
        "    base = Dataset(documentation('_'))\n",
        "\n",
        "    subsets = {}\n",
        "\n",
        "    domains = [\n",
        "        ('lifestyle',),\n",
        "        ('recreation',),\n",
        "        ('science',),\n",
        "        ('technology',),\n",
        "        ('writing',),\n",
        "        ('pooled',),\n",
        "    ]\n",
        "\n",
        "    for (domain,) in domains:\n",
        "        for split in ['dev', 'test']:\n",
        "            corpus = TsvDocs(RelativePath(base_dlc, f'lotte/{domain}/{split}/collection.tsv'), lang='en')\n",
        "            subsets[f'{domain}/{split}'] = Dataset(\n",
        "                corpus,\n",
        "                documentation(f'{domain}/{split}')\n",
        "            )\n",
        "            for qtype in ['search', 'forum']:\n",
        "                subsets[f'{domain}/{split}/{qtype}'] = Dataset(\n",
        "                    corpus,\n",
        "                    TsvQueries(RelativePath(base_dlc, f'lotte/{domain}/{split}/questions.{qtype}.tsv'), lang='en'),\n",
        "                    LotteQrels(RelativePath(base_dlc, f'lotte/{domain}/{split}/qas.{qtype}.jsonl')),\n",
        "                    documentation(f'{domain}/{split}/{qtype}')\n",
        "                )\n",
        "\n",
        "    # ir_datasets.registry.register(NAME, base)\n",
        "    # for s in sorted(subsets):\n",
        "    #     ir_datasets.registry.register(f'{NAME}/{s}', subsets[s])\n",
        "\n",
        "    return base, subsets\n",
        "\n",
        "\n",
        "base, subsets = _init()"
      ],
      "metadata": {
        "id": "wh_D76nHrBxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2020 The HuggingFace Datasets Authors and the current dataset script contributor.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# TODO: Address all TODOs and remove all explanatory comments\n",
        "\"\"\"TODO: Add a description here.\"\"\"\n",
        "\n",
        "\n",
        "import csv\n",
        "import json\n",
        "import os\n",
        "\n",
        "import datasets\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "\n",
        "\n",
        "# TODO: Add BibTeX citation\n",
        "# Find for instance the citation on arxiv or on the dataset repo/website\n",
        "_CITATION = \"\"\"\\\n",
        "@inproceedings{santhanam-etal-2022-colbertv2,\n",
        "    title = \"{C}ol{BERT}v2: Effective and Efficient Retrieval via Lightweight Late Interaction\",\n",
        "    author = \"Santhanam, Keshav  and\n",
        "      Khattab, Omar  and\n",
        "      Saad-Falcon, Jon  and\n",
        "      Potts, Christopher  and\n",
        "      Zaharia, Matei\",\n",
        "    booktitle = \"Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n",
        "    month = jul,\n",
        "    year = \"2022\",\n",
        "    address = \"Seattle, United States\",\n",
        "    publisher = \"Association for Computational Linguistics\",\n",
        "    url = \"https://aclanthology.org/2022.naacl-main.272\",\n",
        "    pages = \"3715--3734\",\n",
        "    abstract = \"Neural information retrieval (IR) has greatly advanced search and other knowledge-intensive language tasks. While many neural IR methods encode queries and documents into single-vector representations, late interaction models produce multi-vector representations at the granularity of each token and decompose relevance modeling into scalable token-level computations. This decomposition has been shown to make late interaction more effective, but it inflates the space footprint of these models by an order of magnitude. In this work, we introduce Maize, a retriever that couples an aggressive residual compression mechanism with a denoised supervision strategy to simultaneously improve the quality and space footprint of late interaction. We evaluate Maize across a wide range of benchmarks, establishing state-of-the-art quality within and outside the training domain while reducing the space footprint of late interaction models by 6{--}10x.\",\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# TODO: Add description of the dataset here\n",
        "# You can copy an official description\n",
        "_DESCRIPTION = \"\"\"\\\n",
        "LoTTE Passages Dataset for ColBERTv2\n",
        "\"\"\"\n",
        "\n",
        "# TODO: Add a link to an official homepage for the dataset here\n",
        "_HOMEPAGE = \"https://huggingface.co/datasets/colbertv2/lotte\"\n",
        "\n",
        "# TODO: Add the licence for the dataset here if you can find it\n",
        "_LICENSE = \"\"\n",
        "\n",
        "_URL = {\n",
        "            \"pooled\": \"https://huggingface.co/datasets/colbertv2/lotte/resolve/main/pooled/\",\n",
        "            \"lifestyle\": \"https://huggingface.co/datasets/colbertv2/lotte/resolve/main/lifestyle/\",\n",
        "            \"recreation\": \"https://huggingface.co/datasets/colbertv2/lotte/resolve/main/recreation/\",\n",
        "            \"science\": \"https://huggingface.co/datasets/colbertv2/lotte/resolve/main/science/\",\n",
        "            \"technology\": \"https://huggingface.co/datasets/colbertv2/lotte/resolve/main/technology/\",\n",
        "            \"writing\": \"https://huggingface.co/datasets/colbertv2/lotte/resolve/main/writing/\"\n",
        "       }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Name of the dataset usually match the script name with CamelCase instead of snake_case\n",
        "class NewDataset(datasets.GeneratorBasedBuilder):\n",
        "    \"\"\"TODO: Short description of my dataset.\"\"\"\n",
        "\n",
        "    VERSION = datasets.Version(\"1.1.0\")\n",
        "\n",
        "    # This is an example of a dataset with multiple configurations.\n",
        "    # If you don't want/need to define several sub-sets in your dataset,\n",
        "    # just remove the BUILDER_CONFIG_CLASS and the BUILDER_CONFIGS attributes.\n",
        "\n",
        "    # If you need to make complex sub-parts in the datasets with configurable options\n",
        "    # You can create your own builder configuration class to store attribute, inheriting from datasets.BuilderConfig\n",
        "    # BUILDER_CONFIG_CLASS = MyBuilderConfig\n",
        "\n",
        "    # You will be able to load one or the other configurations in the following list with\n",
        "    # data = datasets.load_dataset('my_dataset', 'first_domain')\n",
        "    # data = datasets.load_dataset('my_dataset', 'second_domain')\n",
        "    BUILDER_CONFIGS = [\n",
        "        datasets.BuilderConfig(name=\"pooled\", version=VERSION, description=\"\"),\n",
        "        datasets.BuilderConfig(name=\"lifestyle\", version=VERSION, description=\"\"),\n",
        "        datasets.BuilderConfig(name=\"recreation\", version=VERSION, description=\"\"),\n",
        "        datasets.BuilderConfig(name=\"science\", version=VERSION, description=\"\"),\n",
        "        datasets.BuilderConfig(name=\"technology\", version=VERSION, description=\"\"),\n",
        "        datasets.BuilderConfig(name=\"writing\", version=VERSION, description=\"\"),\n",
        "        #datasets.BuilderConfig(name=\"pooled_search_valid\", version=VERSION, description=\"\"),\n",
        "        #datasets.BuilderConfig(name=\"pooled_search_test\", version=VERSION, description=\"\"),\n",
        "        #datasets.BuilderConfig(name=\"pooled_forum_valid\", version=VERSION, description=\"\"),\n",
        "        #datasets.BuilderConfig(name=\"pooled_forum_test\", version=VERSION, description=\"\"),\n",
        "        #datasets.BuilderConfig(name=\"lifestyle_search\", version=VERSION, description=\"\"),\n",
        "        #datasets.BuilderConfig(name=\"lifestyle_forum\", version=VERSION, description=\"\"),\n",
        "        #datasets.BuilderConfig(name=\"recreation_search\", version=VERSION, description=\"\"),\n",
        "        #datasets.BuilderConfig(name=\"recreation_forum\", version=VERSION, description=\"\"),\n",
        "        #datasets.BuilderConfig(name=\"science_search\", version=VERSION, description=\"\"),\n",
        "        #datasets.BuilderConfig(name=\"science_forum\", version=VERSION, description=\"\"),\n",
        "        #datasets.BuilderConfig(name=\"technology_search\", version=VERSION, description=\"\"),\n",
        "        #datasets.BuilderConfig(name=\"technology_forum\", version=VERSION, description=\"\"),\n",
        "        #datasets.BuilderConfig(name=\"writing_search\", version=VERSION, description=\"\"),\n",
        "        #datasets.BuilderConfig(name=\"writing_forum\", version=VERSION, description=\"\"),\n",
        "    ]\n",
        "\n",
        "    DEFAULT_CONFIG_NAME = \"pooled\"  # It's not mandatory to have a default configuration. Just use one if it make sense.\n",
        "\n",
        "    def _info(self):\n",
        "        # TODO: This method specifies the datasets.DatasetInfo object which contains informations and typings for the dataset\n",
        "        features = datasets.Features(\n",
        "            {\n",
        "                \"qid\": datasets.Value(\"int32\"),\n",
        "                \"query\": datasets.Value(\"string\"),\n",
        "                \"author\": datasets.Value(\"string\"),\n",
        "                \"answers\": datasets.Sequence(feature={\"views\": datasets.Value(dtype='int32', id=None),\n",
        "                                                      \"score\": datasets.Value(dtype='int32', id=None),\n",
        "                                                      \"answer_pids\": datasets.Value(dtype='int32', id=None)})\n",
        "            }\n",
        "        )\n",
        "        return datasets.DatasetInfo(\n",
        "            # This is the description that will appear on the datasets page.\n",
        "            description=_DESCRIPTION,\n",
        "            # This defines the different columns of the dataset and their types\n",
        "            features=features,  # Here we define them above because they are different between the two configurations\n",
        "            # If there's a common (input, target) tuple from the features, uncomment supervised_keys line below and\n",
        "            # specify them. They'll be used if as_supervised=True in builder.as_dataset.\n",
        "            # supervised_keys=(\"sentence\", \"label\"),\n",
        "            # Homepage of the dataset for documentation\n",
        "            homepage=_HOMEPAGE,\n",
        "            # License for the dataset if available\n",
        "            license=_LICENSE,\n",
        "            # Citation for the dataset\n",
        "            citation=_CITATION,\n",
        "        )\n",
        "\n",
        "    def _split_generators(self, dl_manager):\n",
        "        # TODO: This method is tasked with downloading/extracting the data and defining the splits depending on the configuration\n",
        "        # If several configurations are possible (listed in BUILDER_CONFIGS), the configuration selected by the user is in self.config.name\n",
        "\n",
        "        # dl_manager is a datasets.download.DownloadManager that can be used to download and extract URLS\n",
        "        # It can accept any type or nested list/dict and will give back the same structure with the url replaced with path to local files.\n",
        "        # By default the archives will be extracted and a path to a cached folder where they are extracted is returned instead of the archive\n",
        "        \n",
        "        _URLS = {\n",
        "            \"forum_dev\": _URL[self.config.name] + \"dev_qas.forum.jsonl\",\n",
        "            \"forum_test\": _URL[self.config.name] + \"test_qas.forum.jsonl\",\n",
        "            \"search_dev\": _URL[self.config.name] + \"dev_qas.search.jsonl\",\n",
        "            \"search_test\": _URL[self.config.name] + \"test_qas.search.jsonl\",\n",
        "        }\n",
        "\n",
        "        downloaded_files = dl_manager.download_and_extract(_URLS)\n",
        "\n",
        "        return [\n",
        "            datasets.SplitGenerator(name=\"forum_dev\", gen_kwargs={\"filepath\": downloaded_files[\"forum_dev\"]}),\n",
        "            datasets.SplitGenerator(name=\"forum_test\", gen_kwargs={\"filepath\": downloaded_files[\"forum_test\"]}),\n",
        "            datasets.SplitGenerator(name=\"search_dev\", gen_kwargs={\"filepath\": downloaded_files[\"search_dev\"]}),\n",
        "            datasets.SplitGenerator(name=\"search_test\", gen_kwargs={\"filepath\": downloaded_files[\"search_test\"]}),\n",
        "        ]\n",
        "\n",
        "    # method parameters are unpacked from `gen_kwargs` as given in `_split_generators`\n",
        "    def _generate_examples(self, filepath):\n",
        "        # TODO: This method handles input defined in _split_generators to yield (key, example) tuples from the dataset.\n",
        "        # The `key` is for legacy reasons (tfds) and is not important in itself, but must be unique for each example.\n",
        "\n",
        "        print(\"Generating examples\")\n",
        "\n",
        "        with open(filepath, encoding=\"utf-8\") as f:\n",
        "\n",
        "            for key, row in enumerate(f):\n",
        "                \n",
        "                data = json.loads(row)\n",
        "\n",
        "                answers = {}\n",
        "\n",
        "                if \"score\" in data.keys():\n",
        "                    answers.update({\"score\": [data['score']]})\n",
        "                else:\n",
        "                    answers.update({\"score\": []})\n",
        "\n",
        "                if \"views\" in data.keys():\n",
        "                    answers.update({\"views\": [data['views']]})\n",
        "                else:\n",
        "                    answers.update({\"views\": []})\n",
        "\n",
        "                answers.update({\"answer_pids\": data['answer_pids']})\n",
        "\n",
        "                if \"question_author\" in data.keys():\n",
        "                    author = data['question_author']\n",
        "                else:\n",
        "                    author = \"\"\n",
        "\n",
        "\n",
        "                yield key, {\n",
        "                        \"qid\": data[\"qid\"],\n",
        "                        \"query\": data[\"query\"],\n",
        "                        \"author\": author,\n",
        "                        \"answers\": answers\n",
        "                }"
      ],
      "metadata": {
        "id": "Rt802h6drFvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D9sZRvJrKMZ",
        "outputId": "841fdedb-553b-469b-86bc-6df0e613bcfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'datasets' from '/usr/local/lib/python3.9/dist-packages/datasets/__init__.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ir_datasets\n",
        "dataset_doc = ir_datasets.load('lotte/lifestyle/dev')\n",
        "# Documents\n",
        "# GenericDoc(doc_id='0', text='The presence of communication amid scientific minds was equa...\n",
        "# GenericDoc(doc_id='1', text='The Manhattan Project and its atomic bomb helped bring an en...\n",
        "# ..."
      ],
      "metadata": {
        "id": "985q8WxvrMhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ir_datasets\n",
        "dataset_query = ir_datasets.load(\"lotte/lifestyle/dev/search\")\n",
        "# namedtuple<query_id, text>"
      ],
      "metadata": {
        "id": "OnC_kqnZrOz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hg1D0W8GwwPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step_2: Using TFIDF to retrieve top 5 documents"
      ],
      "metadata": {
        "id": "24V-dG-Ow7JG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lns8TmAWxJFu",
        "outputId": "44c70349-3a35-40db-8197-f327bc1bc39c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [doc.text for doc in dataset_doc.docs_iter()]\n",
        "doc_ids = [doc.doc_id for doc in dataset_doc.docs_iter()]\n",
        "queries = [query.text for query in dataset_query.queries_iter()]\n",
        "query_ids = [query.query_id for query in dataset_query.queries_iter()]"
      ],
      "metadata": {
        "id": "_jnSviWMx1nE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the tf-idf vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# fit the vectorizer on the documents\n",
        "tfidf_matrix_data = vectorizer.fit_transform(docs)"
      ],
      "metadata": {
        "id": "_qn7-oV_19Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function to retrieve relevant documents using the query\n",
        "def retrieve_documents(query, n=5):\n",
        "    # transform the doc and query into a tf-idf vector\n",
        "    query_vec = vectorizer.transform([query])\n",
        "\n",
        "    # compute the cosine similarity between the query vector and all the document vectors\n",
        "    # scores = tfidf_matrix_data.dot(query_vec.T).toarray().squeeze()\n",
        "\n",
        "    # sort the document IDs based on their scores\n",
        "    # sorted_doc_ids = [doc_id for _, doc_id in sorted(zip(scores, doc_ids), reverse=True)]\n",
        "\n",
        "    # return the top-n relevant documents\n",
        "    # return sorted_doc_ids[:n]\n",
        "\n",
        "    cosine_similarities = cosine_similarity(tfidf_matrix_data, query_vec).flatten()\n",
        "\n",
        "    sorted_doc_ids = [doc_id for _, doc_id in sorted(zip(cosine_similarities, doc_ids), reverse=True)]\n",
        "\n",
        "    return sorted_doc_ids[:n]"
      ],
      "metadata": {
        "id": "msVHbCJLxAvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id = query_ids[0]  # 0th id\n",
        "query = queries[int(id)]\n",
        "\n",
        "print(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWVYN7zc0anx",
        "outputId": "c85a6252-b5ea-4681-b115-21645a2e6782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "how much should i feed my 1 year old english mastiff?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieve_list = retrieve_documents(query)  # Will return top 5 documents having the query \n",
        "print(retrieve_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XYd7xzT6Xks",
        "outputId": "afc13ee0-2e90-4e7f-a29c-bcd032dc9898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['4619', '42913', '54068', '55175', '49783']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in (retrieve_list):\n",
        "  print(i,\":\", docs[int(i)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5YlMLQy1G64",
        "outputId": "0cacc368-7876-4059-aad5-c2e6216955d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4619 : I have a 2 1/2 year old bull mastiff. I have been feeding him Blue Buffalo since I got him at 8 weeks old. He is very lean and active for a bull mastiff. I feed him about 3-4 cups twice a day which averages about 130.00 a month. It is very important that you can afford this breed. I just had to take mine to the vet because he developed some sort of allergies on his skin, eyes and ears and the vet bill was $210.00 with all his medication. This wasn't an option I had to take him an get all his meds or he would have gotten worse. They're just like your children, you can expect things to come up and you need to be able to care for them.\n",
            "42913 : I think the key is to provide as much language exposure as possible, but in a natural way. Natural sources could be enrolling the child in an English-speaking Kindergarten, TV shows in English, frequently having English-speaking visitors or babysitters, etc. If you don't have any natural sources of English, then it's up to you alone. I'm faced with a similar situation in regard to learning English specifically. I'm in a German-speaking country and nobody ever uses English. My toddler is currently learning both his parents' languages so we will add English later. I believe that even very small children can tell the difference between languages, so it would be okay to speak Portuguese with your son some of the time, and English some of the time -- but I also believe that you must be native speaker, or equal to it: you've got to know all the words you're ever going to need! I would also provide English-language materials, like books, DVDs, and toys. If you like electronic, talking toys then consider getting one that talks English. When you start showing DVD's like Bob the Builder or the Barbapapas, why not have them in English rather than in Portuguese? Later, when computers become interesting, only have English software.\n",
            "54068 : As a young person (22 year old), who has many interests and has learned English in college, I can say that if I knew English in my high school years, I would follow my interests rather than playing games, but I did not know English, any this prevent me from doing anything that I was interested in, so the long story short, learning English should be a part of basic things that you teach to your kid because, as it is pointed out in the other answers, English is the most common language in the world, so without knowing English, you basically stuck with the limitations of your country in such a century. Moreover, I must point out that, while I was learning English, the previous exposure to the language (due to games) helped me a lot while talking and reading fluently, so a prior exposure to a second language, I think, helps a lot while learning that language.\n",
            "55175 : Very respectfully have to disagree 100%. Your child will pick up English faster than you can imagine. He/she will then immediately begin to lose French on a daily basis. He/she will soon be annoyed when you speak french to him/her. 20 years later, he/she will meet french people and be hesitant about speaking french to them. TLDR; Use it or lose it. I immigrated to an english speaking country when I was 7. I will happily challenge any native english speaker in their command of the english language. I often correct my native english speaking wife's english. However, I struggle to speak my native non-english language. I now have two kids who went to a day care that is in the my native non-english language. They only speak english with my wife. I try my best to speak my native language to them as much as I can. My son, who is now in 1st grade, completely resents my foreign language. My daughter, who is finishing up day care this year, already defaults to English. Her friends at school practice their English with her. Maybe this will be down-voted, and maybe you'll take the majority's advice, but I will promise you this. If you don't put in extra effort now to have your child get as much of your native language as possible, they will lose it. I think about it like this. For them to be perfectly bilingual, they need to have each language 50% of the time. In school, with friends, on the radio, on tv, etc, they will be exclusively using English. When will they be using French?\n",
            "49783 : The original poster has reason to be worried. I am a native speaker of English. My slightly younger sister is also a native speaker of English. My third-grade class was bilingual Spanish-English. The school made sure to provide English and Math instruction that matched my abilities. I did fine. Later on, when I did study Spanish, I best learned the concepts that had been taught in the bilingual class. My sister's second-grade class was bilingual Spanish-English. She was lost during much of the instruction, and had no way to get the English-language help she needed. She developed a fear of math. She did not make as much progress in English, history, or science as she would have in an English-language classroom.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EyAsbGOm2o3Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}